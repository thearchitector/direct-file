# Infrastructure and Deployment

## Local Development Environment

DirectFile uses Docker Compose to orchestrate all services for local development. The entire system can be started with a single command:

```bash
docker compose up -d --build
```

### Default Services Started

| Container | Service | Port | Purpose |
|-----------|---------|------|---------|
| `direct-file-db` | PostgreSQL | 5435 | Main backend database |
| `state-api-db` | PostgreSQL | 5433 | State API database |
| `email-service-db` | PostgreSQL | 5434 | Email service database |
| `direct-file-api` | Backend API | 8080 | Main REST API |
| `direct-file-csp-simulator` | CSP Simulator | 5000 | Authentication simulator |
| `direct-file-email-service` | Email Service | - | Email relay |
| `state-api` | State API | 8081 | State data transfer |
| `localstack` | AWS Mock | 4566 | S3, SQS, KMS, SNS, IAM, Lambda |
| `redis` | Redis | 6379 | Session cache |
| `direct-file-screener` | Screener | 3500 | Eligibility screener (Astro SSG) |
| `wiremock` | WireMock | 5002 | External service mocking |

### Services NOT in Default Docker Compose

The MeF submit and status services are NOT included in the default configuration because they require real MeF credentials:
- `mef-submit` (port 8083) — Must be started separately
- `mef-status` (port 8082) — Must be started separately

### Optional Monitoring Stack

Enabled via the `monitoring` profile:
```bash
JAVA_TOOL_OPTIONS="-javaagent:/opentelemetry-javaagent.jar" docker compose --profile monitoring up -d --build
```

This adds:
- **OpenTelemetry Collector** (port 8888/8889) — Collects metrics and traces
- **Prometheus** (port 9090) — Time-series metrics storage
- **Grafana** (port 3030) — Dashboarding (default login: admin/directfile)

## Recommended Development Workflow

The team recommends a hybrid approach for day-to-day development:

### Backend Development
1. Run database and localstack in Docker: `docker compose up -d db localstack redis`
2. Run the Spring Boot backend locally via IDE or `./mvnw spring-boot:run`
3. This allows IDE debugging, hot reload, and faster iteration

### Frontend Development
1. Run the backend in Docker: `docker compose up -d api`
2. Run the frontend locally: `npm run start` (in df-client directory)
3. Access at `http://localhost:3000` — supports hot module replacement
4. The Vite dev server provides fast rebuilds and HMR

### Full Stack in Docker
1. `docker compose up -d --build` starts everything
2. Access through CSP simulator at `http://localhost:5000`
3. The CSP simulator routes `/df/file` paths to the frontend and `/df/file/api` paths to the backend

## AWS Services Used in Production

While local development uses LocalStack, production deployments use real AWS services:

| AWS Service | Purpose |
|------------|---------|
| **S3** | Storage for filed tax return artifacts (PDFs, XML) |
| **SQS** | Message queues between services (dispatch, status change, email, etc.) |
| **KMS** | Key management for envelope encryption of taxpayer data |
| **SNS** | Notifications (limited use) |
| **IAM** | Service authentication and authorization |
| **Lambda** | Serverless functions (limited use) |

### S3 Bucket Structure
```
direct-file-taxreturns/
  └── {taxYear}/
      └── taxreturns/
          └── {taxReturnId}/
              └── submissions/
                  ├── {submissionId}.xml    # MeF XML
                  └── {submissionId}.json   # Submission metadata

operations-jobs/
  └── (operational job artifacts)
```

### SQS Queue Setup
LocalStack initializes queues via a startup script (`docker/localstack/localstack-script.sh`). The queues include:
- `dispatch-queue` + `dlq-dispatch-queue`
- `submission-confirmation-queue` + `dlq-submission-confirmation-queue`
- `pending-submission-queue` + `dlq-pending-submission-queue`
- `status-change-queue` + `dlq-status-change-queue`
- Various email notification queues + their DLQs

Dead letter queues (DLQs) capture messages that fail processing after a configurable number of retries (typically 2).

## Environment Variables

### Critical Configuration Variables

| Variable | Purpose | Example |
|---------|---------|---------|
| `LOCAL_WRAPPING_KEY` | Encryption key for local dev (base64 AES key) | Auto-generated by `local-setup.sh` |
| `MEF_SOFTWARE_ID` | MeF software identification | Assigned by IRS |
| `SUBMIT_EFIN` / `STATUS_EFIN` | Electronic Filing Identification Number | 6-digit number |
| `SUBMIT_ETIN` / `STATUS_ETIN` | Electronic Transmitter Identification Number | Assigned by IRS |
| `SUBMIT_ASID` / `STATUS_ASID` | Application System Identification | Assigned by IRS |
| `DF_TIN_VALIDATION_ENABLED` | Enable SSN/TIN validation | `false` for local dev |
| `DF_EMAIL_VALIDATION_ENABLED` | Enable email validation | `false` for local dev |
| `SUBMIT_ID_VAR_CHARS` | Variable characters in submission IDs | `zz` for dev |
| `GIT_COMMIT_HASH` | Current git commit for version tracking | `$(git rev-parse --short main)` |

### Port Configuration
All ports are configurable via environment variables (see docker-compose.yaml for full list). Defaults are chosen to avoid common conflicts.

## Database Migrations

DirectFile uses **Liquibase** for database schema management:

### How Migrations Work
- Migration definitions are stored in `src/main/resources/db/changelog.yaml` and `migrations/` directory
- Each migration is a `changeSet` with a unique ID, author, and description
- Migrations are applied automatically when the application starts
- Liquibase tracks applied migrations in `databasechangelog` and `databasechangeloglock` tables

### Creating New Migrations
```yaml
- changeSet:
    id: sample-changeset
    author: directfile
    comment: sample changeset adding a column
    changes:
      - addColumn:
          tableName: users
          columns:
            - column:
                name: something_new
                type: TEXT
                remarks: this is a new column
    rollback:
      - dropColumn:
          tableName: users
          columns:
            - column:
                name: something_new
```

Every changeset should include a `rollback` block for reversibility.

### Rolling Back
```bash
./mvnw liquibase:rollback -Dliquibase.rollbackCount=1
```

## Build Pipeline

### Shared Dependencies
Before building any individual service, shared libraries must be installed:
```bash
cd direct-file/libs
INSTALL_MEF=1 ../scripts/build-dependencies.sh
```

This builds and installs shared Maven packages that multiple services depend on (fact graph libraries, common utilities, etc.).

### Build Order
1. **Shared libs** (`libs/`) — Common Maven packages
2. **Fact Graph Scala** (`fact-graph-scala/`) — Compile + transpile
3. **Backend API** (`backend/`) — Compile + package
4. **Submit/Status** (`submit/`, `status/`) — Compile + package
5. **Email Service** (`email-service/`) — Compile + package
6. **State API** (`state-api/`) — Compile + package
7. **Frontend** (`df-client/`) — npm install + build
8. **Screener** (`df-client/df-static-site/`) — npm install + build

### Code Quality Tools

| Tool | Scope | Purpose | Command |
|------|-------|---------|---------|
| **Spotless** | Java | Code formatting | `./mvnw spotless:check` / `spotless:apply` |
| **Prettier** | Frontend | Code formatting | Pre-commit hook |
| **SpotBugs** | Java (compiled) | Static analysis / security | `./mvnw spotbugs:check` |
| **PMD** | Java (source) | Static analysis / linting | `./mvnw pmd:check` |
| **ESLint** | Frontend | JavaScript/TypeScript linting | `npm run lint` |
| **Stylelint** | Frontend | CSS/SASS linting | Integrated with build |
| **scalafmt** | Scala | Code formatting | `sbt scalafmt` |
| **pre-commit** | All | Git hooks for formatting/linting | `pre-commit install` |

### Pre-commit Hooks
The repository uses the `pre-commit` framework for git hooks:
```bash
pre-commit install
pre-commit install --hook-type pre-push
```

These hooks run formatting and linting checks before allowing commits and pushes.

## Monitoring and Observability

### OpenTelemetry
All Java services are instrumented with the OpenTelemetry Java agent:
- Automatic instrumentation of HTTP requests, database queries, etc.
- Metrics exported to the OTel Collector
- From there, metrics flow to Prometheus for storage

### Application Logging
- **Production**: JSON-formatted logs (for log aggregators like Splunk)
- **Development**: Standard Spring Boot log format (via `debug` profile)
- **Logback configurations**: Multiple configs for different environments (`logback.xml`, `logback-local.xml`, `logback-debug.xml`, `logback-minimal.xml`)

### Health Checks
- Backend health: `http://localhost:8080/df/file/api/actuator/health`
- State API health: `http://localhost:8081/actuator/health`
- Database health: PostgreSQL `pg_isready` checks (10s interval, 5 retries)

### API Documentation
- Backend Swagger: `http://localhost:8080/df/file/api/swagger-ui/index.html`
- State API Swagger: `http://localhost:8081/swagger-ui/index.html`

## Production Deployment Considerations

### Scaling
- **Backend API**: Can scale to 96 pods max (48 per region) with horizontal autoscaling
- **Status Service**: Single pod per region (identified as a bottleneck — see Status Service 2.0 RFC)
- **State API**: Independent scaling from the main backend

### Security
- All containers run read-only (`read_only: true`)
- Temporary files use tmpfs mounts
- Containers have minimal privileges
- Network segmentation between services

### Branching Strategy
The team adopted a production branching strategy (documented in `2024-11-21_production-branching-strategy.md`) for managing releases and hotfixes across environments.

## Key Takeaways for a Replacement System

1. **Docker Compose for local dev is essential**: Being able to spin up the entire system with one command dramatically reduces onboarding time and development friction.

2. **Use managed services in production, mock them locally**: LocalStack provides faithful mocking of AWS services, allowing offline development while using real AWS in production.

3. **Separate databases per service**: Enforces data isolation and allows independent scaling/management.

4. **Automated schema migrations**: Liquibase (or equivalent like Alembic for Python) ensures database changes are tracked, reversible, and automatically applied.

5. **Invest in observability from day one**: OpenTelemetry + Prometheus + Grafana provides comprehensive monitoring without vendor lock-in.

6. **Design for horizontal scaling**: Avoid single-pod services in the critical path (the Status service bottleneck was a hard lesson).
